{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "\n",
    "Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.\n",
    "\n",
    "The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\n",
    "\n",
    "In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found on Kaggle : \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import itertools as it\n",
    "import pickle\n",
    "import os\n",
    "from  pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                  # module for stop words that come with NLTK\n",
    "from nltk.stem.wordnet import WordNetLemmatizer    # module for lemmatization\n",
    "from nltk import word_tokenize, pos_tag            # tokenization and Part of Speech tagging\n",
    "\n",
    "nltk.download('stopwords') #stopwords used to preprocess the corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english') # a list of English stopwords\n",
    "\n",
    "# Lemmatizer = lemmatizer = WordNetLemmatizer()  # a method that returns the lemmatized form of word \n",
    "#                                                # (\"was\" => \"be\" - \"rocks\" => \"rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File descriptions\n",
    "* train.csv - the training set, contains comments with their binary labels\n",
    "* test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand \n",
    "labeling, the test set contains some comments which are not included in scoring.\n",
    "* sample_submission.csv - a sample submission file in the correct format\n",
    "* test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Data/train.csv\")\n",
    "test_data = pd.read_csv('Data/test.csv')\n",
    "test_label_data = pd.read_csv('Data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the training dataset contains :\n",
    "* the comment ID\n",
    "* the raw text\n",
    "* the different categories of toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "---------------\n",
      "D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "---------------\n",
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "---------------\n",
      "\"\n",
      "More\n",
      "I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
      "\n",
      "There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n",
      "---------------\n",
      "You, sir, are my hero. Any chance you remember what page that's on?\n",
      "---------------\n",
      "\"\n",
      "\n",
      "Congratulations from me as well, use the tools well.  · talk \"\n",
      "---------------\n",
      "COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
      "---------------\n",
      "Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.\n",
      "---------------\n",
      "Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\n",
      "---------------\n",
      "alignment on this subject and which are contrary to those of DuLithgow\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Let's check some comments\n",
    "for i in range(10):\n",
    "    print(train_data['comment_text'][i])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
       "5  0001ea8717f6de06  Thank you for understanding. I think very high...\n",
       "6  00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...\n",
       "7  000247e83dcc1211                   :Dear god this site is horrible.\n",
       "8  00025358d4737918  \" \\n Only a fool can believe in such numbers. ...\n",
       "9  00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check in the test.csv\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just have the ID's and comments with no classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
       "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
       "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
       "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
       "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
       "5  0001ea8717f6de06      0             0        0       0       0   \n",
       "6  00024115d4cbde0f     -1            -1       -1      -1      -1   \n",
       "7  000247e83dcc1211      0             0        0       0       0   \n",
       "8  00025358d4737918     -1            -1       -1      -1      -1   \n",
       "9  00026d1092fe71cc     -1            -1       -1      -1      -1   \n",
       "\n",
       "   identity_hate  \n",
       "0             -1  \n",
       "1             -1  \n",
       "2             -1  \n",
       "3             -1  \n",
       "4             -1  \n",
       "5              0  \n",
       "6             -1  \n",
       "7              0  \n",
       "8             -1  \n",
       "9             -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is filled with ID and classification.  \n",
    "A lot of rows are filled with -1, which means that these id's were not used for scoring (check Kaggle page for more information https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use this dataset to define the accuracy of our models\n",
    "test_label_data = test_label_data.loc[test_label_data['toxic']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic           0.095189\n",
       "severe_toxic    0.005736\n",
       "obscene         0.057692\n",
       "threat          0.003298\n",
       "insult          0.053565\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate the % of toxic comments\n",
    "test_label_data.iloc[:,1:-1].sum(axis=0) / test_label_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a few of toxic comments compare to the dataset size.  \n",
    "However it will be enough to test our model, because this dataset is different from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000663aff0fffc80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this other one from 1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000689dd34e20979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000844b52dee5f3f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00091c35fa9d0465</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000968ce11f5ee34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0001ea8717f6de06      0             0        0       0       0   \n",
       "1  000247e83dcc1211      0             0        0       0       0   \n",
       "2  0002f87b16116a7f      0             0        0       0       0   \n",
       "3  0003e1cccfd5a40a      0             0        0       0       0   \n",
       "4  00059ace3e3e9a53      0             0        0       0       0   \n",
       "5  000663aff0fffc80      0             0        0       0       0   \n",
       "6  000689dd34e20979      0             0        0       0       0   \n",
       "7  000844b52dee5f3f      0             0        0       0       0   \n",
       "8  00091c35fa9d0465      1             0        0       0       0   \n",
       "9  000968ce11f5ee34      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                       comment_text  \n",
       "0              0  Thank you for understanding. I think very high...  \n",
       "1              0                   :Dear god this site is horrible.  \n",
       "2              0  \"::: Somebody will invariably try to add Relig...  \n",
       "3              0  \" \\n\\n It says it right there that it IS a typ...  \n",
       "4              0  \" \\n\\n == Before adding a new product to the l...  \n",
       "5              0                           this other one from 1897  \n",
       "6              0  == Reason for banning throwing == \\n\\n This ar...  \n",
       "7              0             |blocked]] from editing Wikipedia.   |  \n",
       "8              0  == Arabs are committing genocide in Iraq, but ...  \n",
       "9              0  Please stop. If you continue to vandalize Wiki...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's group comments and classification with ID's\n",
    "test = test_label_data.merge(test_data, on='id', how=\"inner\")\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000968ce11f5ee34</td>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text\n",
       "22  000968ce11f5ee34  Please stop. If you continue to vandalize Wiki..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it worked\n",
    "# We take id's from test df and we check comments in test_data\n",
    "id = \"000968ce11f5ee34\"\n",
    "test_data.loc[test_data[\"id\"] == id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000689dd34e20979</td>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text\n",
       "17  000689dd34e20979  == Reason for banning throwing == \\n\\n This ar..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it worked\n",
    "# We take id's from test df and we check comments in test_data\n",
    "id = \"000689dd34e20979\"\n",
    "test_data.loc[test_data[\"id\"] == id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the list of punctuations we want to remove\n",
    "# Note that we let ! in the corpus\n",
    "# punc = '''()-[]{};:'\"\\,<>./?@#$%^&*_~'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let''s define a function that preprocesses a text\n",
    "\n",
    "def preprocess(corpus):\n",
    "    \n",
    "    '''\n",
    "    From a string, make text lowercase, remove hyperlinks, punctuation, word containing numbers, stopwords.\n",
    "    Input : a list of strings\n",
    "    Output : a list of tokens stored in a generator (yield)\n",
    "    '''\n",
    "\n",
    "    for text in corpus:\n",
    "\n",
    "        text = text.lower()                                               # Lowercase\n",
    "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)                   # Remove links\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)   # Remove punctuation\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)                               # Remove words containing numbers\n",
    "    \n",
    "        yield ' '.join([word for word in text.split(' ') if word not in stopwords_english]) # Return a generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We save the cleaned comments in a list to be easily manipulated\n",
    "clean_comments = list(preprocess(train_data['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation\n",
      "why edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired \n",
      "------------\n",
      "daww matches background colour im seemingly stuck thanks  talk  january   utc\n",
      "------------\n",
      "hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info\n",
      "------------\n",
      "\n",
      "more\n",
      "i cant make real suggestions improvement  wondered section statistics later subsection types accidents  think references may need tidying exact format ie date format etc later noone else first  preferences formatting style references want please let know\n",
      "\n",
      "there appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport  \n",
      "------------\n",
      "sir hero chance remember page thats\n",
      "------------\n",
      "\n",
      "\n",
      "congratulations well use tools well  · talk \n",
      "------------\n",
      "cocksucker piss around work\n",
      "------------\n",
      "vandalism matt shirvington article reverted  please dont banned\n",
      "------------\n",
      "sorry word nonsense offensive anyway im intending write anything articlewow would jump vandalism im merely requesting encyclopedic one use school reference selective breeding page almost stub points animal breeding short messy article gives info must someone around expertise eugenics \n",
      "------------\n",
      "alignment subject contrary dulithgow\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(clean_comments[i])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note some words with no meaning, or typos. It can be better but we are going to work with that at first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We do the same for the test set\n",
    "test_clean_comments = list(preprocess(test[\"comment_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will try out several word enbeddings methods. \n",
    "\n",
    "Let's use the simplest one : bag-of-words (BOW)\n",
    "\n",
    "BOW method calculates the frequency of a word for each document, based on a Vocabulary.  \n",
    "To use BOW, instead of recreating from scratch, we can use the library Scikit Learn => CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Bag-of-words\n",
    "vectorizer = CountVectorizer(min_df=3,max_df=0.9) #Filter words that are note present at least in min_df documents & no more that 90% of all documents\n",
    "bow = vectorizer.fit_transform(clean_comments) #return a document-term matrix (n_samples,n_features)\n",
    "bow_test = vectorizer.transform(test_clean_comments) # We do the same for test set, we just transform to have the same number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 52731), (63978, 52731))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape , bow_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated a sparse matrix, composed of word frequencies for each document, with only a small number of non-zero elements (*stored elements* in the representation  below).  \n",
    "\n",
    "*Note*  \n",
    "It appears that using a generator (produced with yield in the preprocessing function *preprocess*) accelerate the preprocessing, but *fit_transform* actually takes more time this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaand' 'aac' 'aachen' 'aah' 'aaliyah' 'aamir' 'aan' 'aand'\n",
      " 'aang' 'aap' 'aaps' 'aar' 'aardvark' 'aarem' 'aaron' 'aarons' 'aas'\n",
      " 'aatalk' 'aau' 'aave' 'ab' 'aba' 'aback' 'abad' 'abaddon' 'abandon'\n",
      " 'abandoned' 'abandoning']\n",
      "------------\n",
      "['abolish' 'abolished' 'abolishing' 'abolition' 'abolitionist'\n",
      " 'abolitionists' 'abomb' 'abominable' 'abomination' 'abominations'\n",
      " 'aboriginal' 'aboriginals' 'aborigine' 'aborigines' 'abort' 'aborted'\n",
      " 'abortion' 'abortions' 'abot' 'abotu' 'abou' 'aboumekhael' 'abound'\n",
      " 'abounds' 'abour' 'about' 'aboutcom' 'abouth' 'abouti' 'above']\n",
      "------------\n",
      "['agreement' 'agreements' 'agrees' 'agress' 'agressing' 'agression'\n",
      " 'agressive' 'agressively' 'agressor' 'agricultural' 'agriculture'\n",
      " 'agriculturists' 'agrizoophobia' 'aground' 'ags' 'aguilera' 'aguri' 'agw'\n",
      " 'ah' 'aha' 'ahaha' 'ahahahahaha' 'aharon' 'ahd' 'ahead' 'ahem' 'ahh'\n",
      " 'ahhh' 'ahhhh' 'ahhrelief']\n",
      "------------\n",
      "['copright' 'coproduced' 'coproducer' 'cops' 'coptic' 'copts' 'copula'\n",
      " 'copulation' 'copy' 'copyandpaste' 'copyandpasting' 'copycat' 'copyedit'\n",
      " 'copyedited' 'copyediting' 'copyeditor' 'copyeditors' 'copyedits'\n",
      " 'copying' 'copyjpg' 'copyleft' 'copypaste' 'copypasted' 'copypasting'\n",
      " 'copyright' 'copyrightable' 'copyrighted' 'copyrightfree'\n",
      " 'copyrightholder' 'copyrightholders']\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the features / vocabulary\n",
    "print(vectorizer.get_feature_names_out()[:30])\n",
    "print('------------')\n",
    "print(vectorizer.get_feature_names_out()[100:130])\n",
    "print('------------')\n",
    "print(vectorizer.get_feature_names_out()[1000:1030])\n",
    "print('------------')\n",
    "print(vectorizer.get_feature_names_out()[10000:10030])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some words are not in the english dictionary, or are just grostesque.  \n",
    "However filtering this word would filter \"toxic\" words too, as \"cocksucker\".  \n",
    "We will therfore let these outliers in the dataset for the moment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define target, which is the classification made by human\n",
    "target = train_data[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]\n",
    "# target = np.array(target) #transform dataframe into array\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if target values are balanced.   \n",
    "In other words, is the target made of as much toxic as non-toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095844\n",
       "severe_toxic     0.009996\n",
       "obscene          0.052948\n",
       "threat           0.002996\n",
       "insult           0.049364\n",
       "identity_hate    0.008805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.sum(axis=0) / target.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the target set  is not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095189\n",
       "severe_toxic     0.005736\n",
       "obscene          0.057692\n",
       "threat           0.003298\n",
       "insult           0.053565\n",
       "identity_hate    0.011129\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "test[keys].sum(axis=0) /test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create the train and test sets using train_test_split\n",
    "# train_x, test_x, train_y, test_y = train_test_split(bow,target, test_size=0.20 ,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes & Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define NaÏve Bayes relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probNB(bow,target,cat):\n",
    "\n",
    "    '''\n",
    "    Naive Bayes probability for each word\n",
    "    Inputs :\n",
    "    bow : bag of words (with doc in rows and words in columns)\n",
    "    target : classification vector (filled with 1 and 0)\n",
    "    cat : 1 or 0, in target\n",
    "    Output : \n",
    "    Vector of Naive Bayes probabilities with smoothing (n_words,1)\n",
    "    '''\n",
    "\n",
    "    p = np.array(bow[target==cat].sum(axis=0))\n",
    "\n",
    "    return np.transpose((p+1) / (p.sum() + bow.shape[1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(bow,target):\n",
    "\n",
    "    '''\n",
    "    Function that return the log likelihood of a document\n",
    "    Inputs :\n",
    "    bow : bag of words (n_doc,n_words)\n",
    "    target : classification of comments (n_doc,1)\n",
    "    Output : \n",
    "    Return a vector of Log Likelihood for each comment (Naïve Bayes) (n_doc,1)\n",
    "    '''\n",
    "\n",
    "    log = np.log(probNB(bow,target,1)/probNB(bow,target,0))\n",
    "    m = bow.dot(log)\n",
    "    model = LogisticRegression().fit(m,target)\n",
    "    return model , log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "Accuracy : 0.9279\n",
      "----\n",
      "fit severe_toxic\n",
      "Accuracy : 0.9938\n",
      "----\n",
      "fit obscene\n",
      "Accuracy : 0.9507\n",
      "----\n",
      "fit threat\n",
      "Accuracy : 0.9961\n",
      "----\n",
      "fit insult\n",
      "Accuracy : 0.9503\n",
      "----\n",
      "fit identity_hate\n",
      "Accuracy : 0.9885\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "df_classification = pd.DataFrame() #We store probabilities into a Dataframe\n",
    "df_classification['Comments'] = test['comment_text']\n",
    "\n",
    "for i,j in enumerate(target.columns):\n",
    "    print('fit', j)\n",
    "    model,log = get_model(bow,target[j])\n",
    "    df_classification[j] = model.predict(bow_test.dot(log))\n",
    "\n",
    "    #Acurracy\n",
    "    score = model.score(bow_test.dot(log) , test[j])\n",
    "    print(f\"Accuracy : {score:.4}\")\n",
    "    print('----')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see where our model was wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the algorithm to all comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the differences between our model results and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesbian bizzle who knows no shizzle\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 0, 0, 1, 1]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "::::Fuck. I misread the dates. Umm. Sorry.\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 0, 0]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "How dare you say I vandalize! >=( you will PAY! $W%$##! #$%^&!! #^&*(!! $%!@@#@@#! You !@##@@ homosexual gay fag homo gay homo\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 1, 1]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n",
      "i fucked your mum\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 0, 0]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n",
      "== THIS IS CHARLIE 12 27 == \n",
      "\n",
      " STOP BANNING ME FAGGS!!!!!!!!!!!!! IM THE COOOLESTE AND YORE ALLGAY FUCKES!!!!! SO GO TE HELL!!!!!!!!!!!!!!11111LOOSER S !!!!!!! 111\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 1, 0]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n",
      "== Penis Airways == \n",
      "\n",
      " FLY PENIS AIRWAYS!!!!! YEEEEY PENISTOWN!!\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 0, 0, 0, 0]\n",
      "Model [1, 0, 1, 0, 0, 0]\n",
      "\n",
      "THIS IS TOM YA SOOOO AWESOME LOLOLOLOLOLOLOLOLOOLOLOLOLOL SOOOOOO WAT YOU PEOPLE DOING U STUPID YOU NEVER GUESSS WHERE I BEEN DR ZOID BERG LOL C MAN\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 0, 0, 1, 0]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n",
      "damn, the wikipedos were really on the ball with this one. good job, guys!\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 1, 0]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n",
      "== U R... == \n",
      "\n",
      " gay like justin bieber except jb is good and u r bad like a honda cuz a honda is not very bad but u r not so ya u r stupid why did you transfrom my page i mean i made it so u had on rite to just tak it awywa u y r a bad prson and i will hunt down u and reat my shit adn you should find revolushon cuz god is always waching ur car and u so yeah stop sining u r a ashole k?\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 1, 1]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n",
      "== who iz tha author ov diz acticle!!?? == \n",
      "\n",
      " i juzz wont 2 kno who it iz ppl!!!!!!!!!!!!\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [0, 0, 0, 0, 0, 0]\n",
      "Model [1, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys = target.columns\n",
    "\n",
    "# List of all comments where our model was wrong \n",
    "# We filter any row that have a difference with the target, located in test\n",
    "\n",
    "# df of True or False for each comment (rows) and class (columns), True means there is a difference\n",
    "df_comparison = df_classification[keys] != test[keys] \n",
    "\n",
    "# We use pd.dataframe.any(axis='columns') that returns True if all True, or False otherwise\n",
    "# In other word, if there is a difference, it returns True for the entire row\n",
    "error_insult_comments = df_classification.loc[df_comparison.any(axis=1)]['Comments']\n",
    "\n",
    "# Get the index list of mismatches\n",
    "index_list = error_insult_comments.index\n",
    "\n",
    "for i in np.random.choice(index_list,10):\n",
    "    print(error_insult_comments[i])\n",
    "    print('---------')\n",
    "    print(f'Classes {list(keys)}')\n",
    "    print(f\"Actual {[test[key][i] for key in keys]}\")\n",
    "    print(f'Model {[df_classification[key][i] for key in keys]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**  \n",
    "\n",
    "As we can, our model misses a lot of classification.  \n",
    "Let's calculate the overall accuracy, which means that any missmatch for a comment is counted as wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is : 0.107\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = df_comparison.any(axis=1).sum(axis=0) / df_comparison.shape[0]\n",
    "print(f'Overall accuracy is : {overall_accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF NB-Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "Accuracy : 0.9162\n",
      "----\n",
      "fit severe_toxic\n",
      "Accuracy : 0.9942\n",
      "----\n",
      "fit obscene\n",
      "Accuracy : 0.9482\n",
      "----\n",
      "fit threat\n",
      "Accuracy : 0.9967\n",
      "----\n",
      "fit insult\n",
      "Accuracy : 0.9471\n",
      "----\n",
      "fit identity_hate\n",
      "Accuracy : 0.9889\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Word embeddings\n",
    "tfidf_vec = TfidfVectorizer(min_df=1,max_df=0.9)\n",
    "tfidf = tfidf_vec.fit_transform(clean_comments)\n",
    "tfidf_test = tfidf_vec.transform(test_clean_comments)\n",
    "\n",
    "# # Split data\n",
    "# # We create the train and test sets using train_test_split\n",
    "# train_x, test_x, train_y, test_y = train_test_split(tfidf,target, test_size=0.20 ,random_state=0)\n",
    "\n",
    "# Let's create our model and calculate accuracy\n",
    "df_classification = pd.DataFrame() #We store probabilities into a Dataframe\n",
    "df_classification['Comments'] = test['comment_text']\n",
    "\n",
    "for i,j in enumerate(target.columns):\n",
    "    print('fit', j)\n",
    "    model,log = get_model(tfidf,target[j])\n",
    "    df_classification[j] = model.predict(tfidf_test.dot(log))\n",
    "\n",
    "    #Acurracy\n",
    "    score = model.score(tfidf_test.dot(log) , test[j])\n",
    "    print(f\"Accuracy : {score:.4}\")\n",
    "    print('----')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Bag Of Words embedding :\n",
    "\n",
    "fit toxic\n",
    "Accuracy : 0.9269\n",
    "\n",
    "fit severe_toxic\n",
    "Accuracy : 0.9931\n",
    "\n",
    "fit obscene\n",
    "Accuracy : 0.9472\n",
    "\n",
    "fit threat\n",
    "Accuracy : 0.9962\n",
    "\n",
    "fit insult\n",
    "Accuracy : 0.9476\n",
    "\n",
    "fit identity_hate\n",
    "Accuracy : 0.9883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "We slightly increase the performance overall using TfIdf embedding, except for toxic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Fuck Dre, tell that bitch he can kiss my ass\"\" \n",
      "\n",
      " http://www.pacandbig.com/2pac/lyrics/Until%20The%20End%20Of%20Time/CD1/02%20-%20Fuck%20Friendz.txt \n",
      "\n",
      " \"\"No longer Dre Day, arrivederci \n",
      " Blown and forgotten, rotten for plottin Child's Play \n",
      " Check your sexuality, as fruity as this Alize \n",
      " Quick to jump ship, punk trick, what a dumb move \n",
      " Cross Death Row, now who you gon' run to? \n",
      " Lookin for suckers cause you similar \n",
      " Pretendin to be hard, oh my God, check your temperature \n",
      " Screamin Compton, but you can't return, you ain't heard \n",
      " Brothers pissed cause you switched and escaped to the burbs \n",
      " Mob on to this new era, cause we Untouchable \n",
      " Still can't believe that you got 'Pac rushin you \n",
      " Up in you, bless the real, all the rest get killed \n",
      " Who can you trust, only time reveals  toss it up!\"\" \n",
      "\n",
      " http://www.pacandbig.com/2pac/lyrics/The%20Don%20Killuminati%20The%207%20Day%20Theory/03%20-%20Toss%20It%20Up.txt \n",
      "\n",
      " \"\"LA, California Love part motherfuckin Two \n",
      " Without gay ass Dre\"\" \n",
      "\n",
      " http://www.pacandbig.com/2pac/lyrics/The%20Don%20Killuminati%20The%207%20Day%20Theory/04%20-%20To%20Live%20&%20Die%20In%20L.A.txt \n",
      "\n",
      " \"\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 1, 0]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "unblock|reason= I did not mean it like that, but its all true, im sorry your being a dick about it but, reconsider my offer and let me edit again, or ill just create another account x3 ♥~ Jason (you are dick)\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 1, 0]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "your incredibly pathetic and childish\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 0, 0, 1, 0]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\" \n",
      "\n",
      " == Thanks for the laugh... == \n",
      "\n",
      " \"\"(→User-reported - manually removing Evilblood4, who has already been blocked (suck it, bot!))\"\".   / \"\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 1, 0, 0, 0]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n",
      ": You know dam well why hes DYK that stupid statistic sourced from a atrociously vague reportthis guy is pushing the envelope of tolerance he wants a reaction from either Topgun or me and he will be getting one soon\n",
      "---------\n",
      "Classes ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Actual [1, 0, 0, 0, 0, 0]\n",
      "Model [0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys = target.columns\n",
    "\n",
    "# List of all comments where our model was wrong \n",
    "# We filter any row that have a difference with the target, located in test\n",
    "\n",
    "# df of True or False for each comment (rows) and class (columns), True means there is a difference\n",
    "df_comparison = df_classification[keys] != test[keys] \n",
    "\n",
    "# We use pd.dataframe.any(axis='columns') that returns True if all True, or False otherwise\n",
    "# In other word, if there is a difference, it returns True for the entire row\n",
    "error_insult_comments = df_classification.loc[df_comparison.any(axis=1)]['Comments']\n",
    "\n",
    "# Get the index list of mismatches\n",
    "index_list = error_insult_comments.index\n",
    "\n",
    "for i in np.random.choice(index_list,5):\n",
    "    print(error_insult_comments[i])\n",
    "    print('---------')\n",
    "    print(f'Classes {list(keys)}')\n",
    "    print(f\"Actual {[test[key][i] for key in keys]}\")\n",
    "    print(f'Model {[df_classification[key][i] for key in keys]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is : 0.118\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = df_comparison.any(axis=1).sum(axis=0) / df_comparison.shape[0]\n",
    "print(f'Overall accuracy is : {overall_accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions on models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if we check the overall accuracy, by considering all classification at the same time, it appears that our model perfoms badly.\n",
    "\n",
    "* 10,7% correct accuracy for BOW + NB-Logistic regression\n",
    "* 11,8% for TF-IDF + Logistic Regression\n",
    "\n",
    "If we look at each classification individually, we note :\n",
    "\n",
    "* 96.1% accuracy in average for BOW\n",
    "* 96.7% accuracy in average for TF-IDF\n",
    "\n",
    "*UPDATE*  \n",
    "After submission on Kaggle the score of TFIDF NB Logistic Regression model is 0.84586!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9606199999999999, 0.9665499999999999)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate classes accuracy mean for bow and TF-IDF\n",
    "np.mean([0.9269,0.9931,0.9472,0.9476,0.9883]) , np.mean([0.9269,0.9931,0.9472,0.9962,0.9476,0.9883])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save models for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to run this code, make the statment true\n",
    "if 0==1:\n",
    "\n",
    "    # TFIDF vectorizer\n",
    "    pickle.dump(tfidf_vec,open('tfidf_vectorizer.pkl','wb'))\n",
    "\n",
    "    # Logistic regression models + Log Likelihood based on Naïve Bayes for each toxicity classification\n",
    "    for i,j in enumerate(target.columns):\n",
    "        print('fit', j)\n",
    "        model,log = get_model(tfidf,target[j])\n",
    "        pickle.dump((model,log),open('models_'+f'{target.columns[i]}.pkl','wb')) #Save models and logs as tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an algorithm that proceeds a comment and return classification of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the folder Models, we import the Logistic model and the Log Likelihood\n",
    "for dirname, _, filenames in os.walk('Models'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxicity(comment):\n",
    "\n",
    "    '''\n",
    "    A function that takes a raw comment as input and returns the a vector of categorical \n",
    "    values as ['identity_hate','insult','obscene','severe_toxic','threat','toxic'].\n",
    "    '''\n",
    "\n",
    "    # Get everything we need for this function\n",
    "    from utils import preprocess\n",
    "    tfidf_vec = pickle.load(open(\"tfidf_vectorizer.pkl\",'rb'))\n",
    "\n",
    "\n",
    "    # Declaration of comment classification as a pd.Series\n",
    "    classification = pd.Series(index=['identity_hate','insult','obscene','severe_toxic','threat','toxic'],dtype=int)\n",
    "    i = 0 # We be used as increment to fill the vector\n",
    "\n",
    "    # Clean comment\n",
    "    clean_comments = preprocess(comment)\n",
    "\n",
    "    # Get word embeddings\n",
    "    word_emb = tfidf_vec.transform(clean_comments)\n",
    "\n",
    "    # From the folder Models, we import the Logistic model and the Log Likelihood, one by one\n",
    "    for dirname, _, filenames in os.walk('Models'):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirname, filename)\n",
    "            model,log = pickle.load(open(path,'rb'))\n",
    "\n",
    "            classification[i] = model.predict(word_emb.dot(log))\n",
    "            i += 1\n",
    "\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = [\"Stupid chinese people!\"]\n",
    "get_toxicity(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess\n",
    "\n",
    "preprocess(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae290cde4634a6443381d55bbe6f1180de26a7de00f3f0eb338834ba70f07938"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
